# -*- coding: utf-8 -*-
"""London_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oBIpTiT2M2BsupY_mv-8IDHHHlwlNLdy

# London Houses Project
---

## Introducci√≥n

PropTech, o la tecnolog√≠a inmobiliaria, ofrece varios beneficios en el mercado inmobiliario. La eliminaci√≥n de intermediarios y la simplificaci√≥n de los procesos son dos de las principales ventajas de las proptech inmobiliarias. Esto hace que sean atractivas para el mercado y ayudan a mejorar la eficiencia y rapidez en las transacciones inmobiliarias.

Las inmobiliarias online tienen muchas ventajas, como el ahorro de tiempo y dinero. La tecnolog√≠a es un ingrediente clave en estas empresas, permitiendo estrategias de marketing inmobiliario y publicidad, tours virtuales y fotograf√≠a profesional para mejorar la experiencia del usuario.

## Tabla de Contenido

1. Descargar los datos
2. Vistazo a la Base de Datos
3. Crear set de entrenamiento y prueba
4. Visualizar los Datos
5. Medir la Correlaci√≥n
6. Combinaci√≥n de Variables
7. Transformaci√≥n de Datos
8. Manejo de texto y valores categ√≥ricos
9. Escalaci√≥n de variables
10. Pipeline
11. Seleccionar y entrenar modelos
12. Afinar el modelo
13. Conclusiones

## Objetivo

Trabajamos para una proptech que quiere saber si podemos utilizar machine learning para predecir el precio de las casas, ya que el m√©todo convencional que ellos utilizan tiene un margen de error de 25%. En este caso se realizar√° un modelo para predecir la media de precios en las viviendas de los diferentes municipos de Londres, Reino Unido üá¨üáß.

## <span style="color:green">1. Descargar los datos</span>

Las bases de datos para este proyecto se puede encontrar en este enlace: https://www.kaggle.com/justinas/housing-in-london

Tambi√©n se pueden consultar todas las bases de datos de este curso en GitHub:https://github.com/a2Proyectos/MachineLearning_Data

- housing_in_london_yearly_variables.csv, con los datos que necesitamos para hacer la regresi√≥n.
- London_Borough_Excluding_MHW.shp, con los datos que necesitamos para graficar Londres.
- Capitulo_2/housing_in_london_monthly_variables.csv, con los datos de la media salarial
"""

## Importamos nuestras librerias principales panda, numpy, matplotlib, os...
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import geopandas as gpd
import requests
import seaborn as sns
## Descomprimir archivos Zip
# from zipfile import ZipFile
# from io import BytesIO

## Quitar notaci√≥n cient√≠fica
pd.options.display.float_format = "{:,.2f}".format

from pandas.plotting import scatter_matrix
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

## Definimos una funci√≥n para extraer datos.
#DOWNLOAD_ROOT es la base del GitHub donde vamos a estar descargando las bases de datos.
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/a2Proyectos/MachineLearning_Data/main/"

#Complementos con la direcci√≥n especifica de la base de datos que queremos.
LONDON_SALARY = "Capitulo_2/housing_in_london_yearly_variables.csv"
LONDON_HOUSING = "Capitulo_2/housing_in_london_monthly_variables.csv"
LONDON_MAP = os.path.abspath("") + "\map\London_Borough_Excluding_MHW.shp"

def extraer_datos(root, database):
  csv_path = root + database
  return pd.read_csv(csv_path)

df1 = extraer_datos(DOWNLOAD_ROOT, LONDON_HOUSING)
df2 = extraer_datos(DOWNLOAD_ROOT, LONDON_SALARY)

"""***

## <span style="color:green">2. Vistazo a la Base de Datos</span>
"""

## Juntamos nuestra base de datos de la media salarial, con la de datos de Londres.
df2 = df2.filter(['median_salary', 'area', 'date'])

#Filtrar los datos
df2.head()

#Fusionar los dos dataframe
data = pd.merge(df2,df1)
data.head()

# Obtener informaci√≥n de los datos.
data.info()

"""En el mundo real, un dataset de 900 filas es muy poca informaci√≥n para un modelo de machine learning, sin embargo, debemos tomar en cuenta que solo estamos realizando un ejercicio sencillo. En cuanto a los datos nulos, lo ideal ser√≠a trabajarlos, sin embargo por ahora vamos a romper esta regla y no los vamos a tocar.

***
"""

# saber que tipo de datos contiene alguna variable de tipo objeto, contado sus valores:
data['area'].value_counts()

# obtener informaci√≥n de nuestros datos num√©ricos:
data.describe()

# Realizar un Histograma para visualizar los datos
data.hist(bins=50, figsize=(15,10))
plt.show()

"""***

## <span style="color:green">3. Crear un set de entrenamiento y de prueba </span>
"""

# Dividir los datos train_test_split
set_ent, set_prueba = train_test_split(
    data,
    test_size = .3,
    random_state=45
    )

## Conformamos el tama√±o de los sets
print(len(set_ent))
print(len(set_prueba))

"""Ya tenemos nuestro set de prueba y nuestro set de entrenamiento ‚úÖ

***

### <span style="color:blue">3.1 Evitar Sesgo </span>
"""

# Para categorizar una variable, ejemplo con 5 niveles.
data['salary_cat']=pd.cut(
                          data['median_salary'],
                          bins=[0,10000,20000,30000,40000,np.inf],
                          labels=[1,2,3,4,5]
                          )

#Hacer un histograma de las categor√≠as
data['salary_cat'].hist()
plt.xlabel('Categor√≠a')
plt.ylabel('Rango')
plt.show();

# Verificar que no existen datos en el bin #1
data['salary_cat'].value_counts()

# Redefinir en 4 categor√≠as, quitando en donde no hay datos (el bin#1)
data = data.dropna(subset=['median_salary'])
data = data.reset_index()

data['salary_cat']=pd.cut(
                          data['median_salary'],
                          bins=[10000,20000,30000,40000,np.inf],
                          labels=[1,2,3,4,]
                          )

#Verifica que si se redefinieron las categor√≠as
data['salary_cat'].hist()
plt.xlabel('Categor√≠a')
plt.ylabel('Rango')
plt.show();

## Dividir datos bas√°ndonos en nuestras categor√≠as de salarios
dividir = StratifiedShuffleSplit(
    n_splits=1,
    test_size=0.3,
    random_state=45
    )

# Generamos nuestro objeto para que lo divida en 30% y solo haga una divisi√≥n
# Creamos nuestras variables bas√°ndonos en nuestras categor√≠as
for ent_index, prueba_index in dividir.split(data,data['salary_cat']):
  cat_set_ent = data.loc[ent_index]
  cat_set_prueba = data.loc[prueba_index]

# Comprobaci√≥n. Ya en porcentaje
cat_set_prueba['salary_cat'].value_counts()

"""***"""

# Creamos dataframe para trabajar con el set de entrenamiento
df = cat_set_ent.copy()
df.head()

"""***

## <span style="color:green">4. Visualizar los Datos Gr√°ficamente </span>


Para esto vamos a necesitar, en conjunto con nuestro dataset LONDON_MAP, una nueva libreria que se llama geopandas, el cual exteiende la libreria pandas, para trabajar con datos geoespaciales, se puede encontrar m√°s informaci√≥n en: https://geopandas.org/getting_started/introduction.html
"""

# Cargar el mapa
# Comprobar la carga del mapa
londres_map = gpd.read_file("/content/London_Borough_Excluding_MHW.shp")
londres_map.head()

# Graficamos el mapa
londres_map.plot();

"""***"""

# Ajustamos los nombres de las columnas para desp√∫es hacer un merge.
#utilizar lower para cambiar de may√∫sculas a min√∫sculas
londres_map.columns = londres_map.columns.str.lower()
#Seleccionar columnas necesarias
londres_map = londres_map.rename(
                                {
                                    "name":"area",
                                    "gss_code":"code"
                                    },
                                 axis=1
                                )
londres_map["area"] = londres_map["area"].str.lower()

londres_map.head()

# Filtramos dataset
# Comprobamos filtro
londres_map = londres_map.filter(items=['area', 'code', 'hectares', 'geometry'])
londres_map.head()

"""***"""

# Seleccionar datos de nuestro set de entrenamiento
df_m = df.groupby('area').agg(
                              {
                                  'average_price':['mean'],
                                  'houses_sold':'sum',
                                  'median_salary':['mean']
                                  }
                              )

# Le asignamos nombre a las columnas del nuevo dataframe y reseteamos el indice
df_m.columns = ['average_price', 'houses_sold', 'median_salary']
df_m.reset_index(inplace=True)
df_m.head()

# Combinar dtaframes
londres_map = pd.merge(londres_map, df_m, on="area")
londres_map.head()

"""***"""

#Gr√°fica del promedio de los precios en las casas
#Cuando se grafica en geopandas hay muchos argumentos, no se desesperen si no los recuerdan, es normal.
plt = londres_map.plot(
                      column='average_price',
                      cmap='Reds',
                      edgecolor='maroon',
                      legend=True,
                      legend_kwds={
                                    'label':'Precio',
                                    'orientation':'horizontal'
                                    }
                      )
plt.set_title('Media de los precios de casas')
plt.axis('off');

"""üí∏: Los precios son mayores en el centro de la cuidad"""

#Graficar ahora el total de las casa vendidas (utilizar el c√≥digo anterior para no repetir)
plt = londres_map.plot(
                      column='houses_sold',
                      cmap='Blues',
                      edgecolor='black',
                      legend=True,
                      legend_kwds={
                                    'label':'Precio',
                                    'orientation':'horizontal'
                                    }
                      )
plt.set_title('Total de Casas Vendidas')
plt.axis('off');

"""Las casas en el norte y sur son de las m√°s vendidas, el centro es el top de ventas."""

plt = londres_map.plot(
                      column='median_salary',
                      cmap='Greens',
                      edgecolor='black',
                      legend=True,
                      legend_kwds={
                                    'label':'Precio',
                                    'orientation':'horizontal'
                                    }
                      )
plt.set_title('Salarios Promedio')
plt.axis('off');

"""Las personas que m√°s ingresos generan viven en la zona centro.

***

## <span style="color:green">5. Medir la Correlaci√≥n </span>
"""

# Crear matriz de correlaci√≥n
matriz = df.corr(method="pearson")
# Comparar correlaci√≥n
matriz['average_price'].sort_values(ascending=False)

# Crear vector
# Graficar
plt = sns.heatmap(
    matriz,
    annot=True,
    cmap="YlGnBu_r"
    )

"""Existe una buena correlaci√≥n entre el precio de las casas y el salario promedio mientras que el n√∫mero de crimenes presenta una correlaci√≥n d√©bil en el precio de una casa."""

#Para graficar scatter_matrix...
columns = ["average_price", "median_salary", "no_of_crimes", "houses_sold"]
scatter_matrix(
    df[columns],
    figsize=(12,13),
    color="#D52B06",
    alpha=0.3,
    hist_kwds={
        'color':['bisque'],
        'edgecolor':'firebrick'
        }
    );

#Por si quieren ver una gr√°fica en espec√≠fico m√°s a detalle
df.plot(kind='scatter', x='median_salary', y='average_price');

"""***

## <span style="color:green">6. Combinaci√≥n de Variables </span>
"""

#Armar una columna para hacer las combinaciones que necesitamos
df["Vendidas_Poblaci√≥n"]=df["no_of_crimes"]/df['houses_sold']
# Crear matriz de correlaci√≥n
matriz = df.corr(method="pearson")
matriz['average_price'].sort_values(ascending=False)

"""***

## <span style="color:Blue">7. Transformaci√≥n de Datos </span>
"""

# Crear Dataframe de predictores y variable a predecir ‚úÇÔ∏è
df_label = cat_set_ent['average_price']
#Nuevo dataframe sin average_price
df_label.head()

# Df train
df = cat_set_ent.drop(
                      'average_price',
                      axis=1
                      )
df.head()

"""***

#### Transformaci√≥n Pandas
"""

df.isna().sum()

# Quitar n√∫mero de cr√≠menes
df = df.drop('no_of_crimes', axis=1)

df.head()

#Tomamos la media
median = df['houses_sold'].median()
df['houses_sold'].fillna(median, inplace=True)
df.isna().sum()

"""#### Transformaci√≥n Scikit"""

# Df train
df = cat_set_ent.drop(
                      'average_price',
                      axis=1
                      )

"""***"""

imputer = SimpleImputer(strategy='median')

df = df.drop(
     'no_of_crimes', axis=1
     )
df_num = df.drop(
    ['area', 'date', 'code'],
    axis=1
    )

imputer.fit(df_num)
X = imputer.transform(df_num)

df_tr = pd.DataFrame(
    X, columns=df_num.columns, index=df_num.index
    )

print(df_tr.head())
print('valores ausentes')
print(df_tr.isna().sum())

"""***

## <span style="color:Blue">8. Manejo de texto y valores categ√≥ricos </span>
"""

#definir que variable vamos a cambiar a valor num√©rico
df_cat = df[['area']]
ordinal_encoder = OrdinalEncoder()
df_oe = ordinal_encoder.fit_transform(df_cat)
ordinal_encoder.categories_

#ONE HOT ENCODER, Convertir variables categ√≥ricas en binarias
encoder = OneHotEncoder()
df_1hot = encoder.fit_transform(df_cat)
df_1hot.toarray()

"""***

## <span style="color:Blue">9. Escalaci√≥n de variables </span>

#### <span style="color:Blue">9.1 Normalizaci√≥n </span>
"""

#Importar MinMaxScaler
scaler = MinMaxScaler()
prueba = pd.DataFrame(
    {'col1': [100,200,300,999],
     'col2': [0,0,1,2],
     'col3':[-10,0,1,2]
     }
    )
pd.DataFrame(scaler.fit_transform(prueba), columns=prueba.columns)

"""#### <span style="color:Blue">9.2 Estandarizaci√≥n </span>"""

## Estandarizaci√≥n (Es el m√°s ultizado)
scaler = StandardScaler()
pd.DataFrame(scaler.fit_transform(prueba), columns=prueba.columns)

"""***

## <span style="color:Blue">10. Pipeline </span>
"""

# Crear funci√≥n de pipeline
pipeline = Pipeline([
    ('rellenar', SimpleImputer(strategy='median')),
    ('escalar', StandardScaler())
])
pd.DataFrame(
    pipeline.fit_transform(
        prueba
        ),
          columns=prueba.columns
    )

# Aplicando Column Transformer
num = list(df_num)
cat = list(df_cat)

#Hacer un datframe que usaremos para la regresi√≥n lineal
pipeline_completo = ColumnTransformer([
    ('num', pipeline, num),
    ('cat', OneHotEncoder(), cat)
])

#Visualiza los datos
df_preparado = pipeline_completo.fit_transform(df)
df_preparado

df_preparado.toarray()

"""***

## <span style="color:purple">11. Seleccionar y entrenar modelos </span>
"""

## Hacer Regresi√≥n Lineal
reg_lin = LinearRegression()
reg_lin.fit(df_preparado, df_label)

#An√°lisis preambultario de los errores
algunos_datos = df.iloc[:5]
dato_predecir = df_label.iloc[:5]

datos_transformados = pipeline_completo.transform(algunos_datos)

vp = reg_lin.predict(datos_transformados)

vr = list(dato_predecir)

vp = pd.Series(vp)
vr = pd.Series(vr)

vr-vp

abs((vr-vp)/vr).mean()

"""***

#### <span style="color:purple">11.1 RMSE </span>
"""

# Calcular el promedio de la suma de los errores al cuadrado RMSE
prediccion = reg_lin.predict(df_preparado)
error = mean_squared_error(df_label, prediccion)
error = np.sqrt(error)
error

#Sacar el promedio
df_label.mean()

#Calcular el porcentaje de acierto
error / df_label.mean()

"""***

#### <span style="color:purple">11.2 √Årbol de Decisi√≥n </span>
"""

# importar DecisionTreeRegressor
reg_arbol = DecisionTreeRegressor()
reg_arbol.fit(df_preparado,df_label)

#calcular el error
prediccion = reg_arbol.predict(df_preparado)
error = mean_squared_error(df_label, prediccion)
error = np.sqrt(error)
error

"""Este cero no significa un excelente resultado, m√°s bien parece un problema de **overfitting**

***

#### <span style="color:purple">11.3 Validaci√≥n Cruzada </span>
"""

#Importar cross_val_score, creo objeto, corro mi funci√≥n
resultados = cross_val_score(
    reg_arbol,
    df_preparado,
    df_label,
    scoring='neg_mean_squared_error',cv=10
    )

#Calcular el porcentaje de acierto de validaci√≥n cruzada
rmse = np.sqrt(-resultados)
rmse

rmse.mean()

"""***

#### <span style="color:purple">11.4 Bosque Aleatorio </span>
¬°Este tambi√©n es importante!
"""

#Importar RandomForestRegressor, creo objeto, corro mi funci√≥n
reg_forest = RandomForestRegressor()
reg_forest.fit(df_preparado,df_label)
#calcular el error
prediccion = reg_forest.predict(df_preparado)

#Calcular el el promedio de la suma de los errores al cuadrado
error = mean_squared_error(df_label,prediccion)
error = np.sqrt(error)
error

#Calcular el porcentaje de acierto de bosque aleatorio
error / df_label.mean()

resultados = cross_val_score(
    reg_forest,
    df_preparado,
    df_label,
    scoring='neg_mean_squared_error',
    cv=10)
rmse = np.sqrt(-resultados)
rmse

rmse.mean()

rmse.mean()/df_label.mean()

"""***

## <span style="color:purple">12. Afinar el modelo </span>

### <span style="color:purple">12.1 Grid Search</span>
"""

param_grid = ({
    'n_estimators':[3,10,10],
    'max_features':[2,4,6,8]
})

grid_search = GridSearchCV(
    reg_forest,
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True
    )

grid_search.fit(df_preparado,df_label)

grid_search.best_params_

np.sqrt(-grid_search.best_score_)/df_label.mean()

"""### <span style="color:purple">12.2 set de prueba</span>"""

#¬°AHORA SI! A utilizar el set de prueba. Es la √∫ltima parte
modelo_final = grid_search.best_estimator_
modelo_final

Y = cat_set_prueba['average_price']
X = cat_set_prueba.drop('average_price',axis=1)

X = X.drop('no_of_crimes', axis=1)
X

X_preparada = pipeline_completo.transform(X)
prediccion_final = modelo_final.predict(X_preparada)

mse_final = mean_squared_error(Y, prediccion_final)
rmse = np.sqrt(mse_final)

# Porcentaje de acierto
print(rmse)
print(rmse/Y.mean())

"""## Conclusiones

En resumen, este proyecto de machine learning ha logrado un avance significativo al reducir el error en la predicci√≥n de precios de viviendas de un 25% a un 20%.

Esta mejora sustancial refleja el impacto positivo de las t√©cnicas y modelos implementados, lo que demuestra el potencial de la **inteligencia artificial** para optimizar la precisi√≥n en la valoraci√≥n de bienes ra√≠ces. Este resultado no solo beneficia a los profesionales del sector inmobiliario, sino tambi√©n a los compradores y vendedores al proporcionar estimaciones m√°s confiables y √∫tiles para la toma de decisiones.
"""